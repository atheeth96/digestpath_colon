{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'RunOptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-634b2cb725d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# from keras.backend.tensorflow_backend import set_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mrun_opts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport_tensor_allocations_upon_oom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#config.gpu_options.per_process_gpu_memory_fraction=0.29\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'RunOptions'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import draw\n",
    "from skimage.io import imread\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import train_test_split#import spams\n",
    "\n",
    "from skimage.util import pad\n",
    "import skimage\n",
    "\n",
    "# import time\n",
    "# import math\n",
    "# import keras\n",
    "# from keras.models import *\n",
    "# from keras.layers import *\n",
    "\n",
    "# from keras.preprocessing import image\n",
    "# import keras.backend as K\n",
    "# import random\n",
    "# from sklearn.metrics import f1_score\n",
    "# from skimage.transform import resize\n",
    "# from skimage.filters import threshold_otsu\n",
    "# from tqdm import tqdm_notebook as tqdm \n",
    "# #from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.29\n",
    "config.gpu_options.allow_growth=True ##to use gpu as needed\n",
    "config.log_device_placement = True  \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_mask'\n",
    "IMAGE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_color_normalized'\n",
    "IMAGE_WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/input'\n",
    "MASK_WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/mask'\n",
    "DISCARD_WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/discard'\n",
    "\n",
    "MASK_PATH_NN='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_mask'\n",
    "IMAGE_PATH_NN='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1'\n",
    "IMAGE_WRITE_PATH_NN='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/input_non_normalized'\n",
    "MASK_WRITE_PATH_NN='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/mask_non_normalized'\n",
    "DISCARD_WRITE_PATH_NN='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/discard_non_normalized'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_patches(MASK_PATH,IMAGE_PATH,IMAGE_WRITE_PATH,MASK_WRITE_PATH,DISCARD_WRITE_PATH):\n",
    "    \n",
    "    if not os.path.exists(IMAGE_WRITE_PATH):\n",
    "        os.mkdir(IMAGE_WRITE_PATH)\n",
    "        print('input directory made')\n",
    "    else:\n",
    "        print('input directory exists')\n",
    "\n",
    "    if not os.path.exists(MASK_WRITE_PATH):\n",
    "        os.mkdir(MASK_WRITE_PATH)\n",
    "        print('mask directory made')\n",
    "    else:\n",
    "        print('mask directory exists')\n",
    "\n",
    "    if not os.path.exists(DISCARD_WRITE_PATH):\n",
    "        os.mkdir(DISCARD_WRITE_PATH)\n",
    "        print('discard directory made')\n",
    "    else:\n",
    "        print('discard directory exists')\n",
    "\n",
    "    image_list=os.listdir(IMAGE_PATH)\n",
    "\n",
    "    for image_ID in tqdm(image_list):\n",
    "        # Format of image_ID : D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942.jpg\n",
    "        image_name=os.path.join(IMAGE_PATH,image_ID)\n",
    "        gray_img=cv2.imread(image_name,0)\n",
    "        bgr_img=cv2.imread(image_name,1)\n",
    "        mask_ID=image_ID.split('.')[0]+'_mask.'+image_ID.split('.')[-1]\n",
    "        # Format of mask_ID: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_mask.jpg\n",
    "        mask_name=os.path.join(MASK_PATH,mask_ID)\n",
    "        mask_img=cv2.imread(mask_name,0)\n",
    "        ret,mask_img=cv2.threshold(mask_img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "\n",
    "        th,img_otsu=cv2.threshold(gray_img,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        temp_x,temp_y=np.where(img_otsu==255)\n",
    "\n",
    "        for count_x,x in enumerate(range(img_otsu.shape[0]//512)):\n",
    "            for count_y,y in enumerate(range(img_otsu.shape[1]//512)):\n",
    "\n",
    "                temp_otsu=img_otsu[count_x*512:(count_x+1)*512,count_y*512:(count_y+1)*512]\n",
    "                temp_mask=mask_img[count_x*512:(count_x+1)*512,count_y*512:(count_y+1)*512]\n",
    "\n",
    "\n",
    "                if len(np.where(temp_otsu==0)[0])<0.6*len(np.where(temp_otsu==255)[0]) and len(np.where(temp_mask==255)[0])>=5000:\n",
    "                    img_write=bgr_img[count_x*512:(count_x+1)*512,count_y*512:(count_y+1)*512,:]\n",
    "\n",
    "                    write_path_image=os.path.join(IMAGE_WRITE_PATH,image_ID.split('.')[0]+\\\n",
    "                                              \"_{}_{}.jpg\".format(count_x,count_y))\n",
    "            # Format of image_ID now: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_<count_x>_<count_y>.jpg\n",
    "                    cv2.imwrite(write_path_image,img_write)\n",
    "                    write_path_mask=os.path.join(MASK_WRITE_PATH,mask_ID.split('_mask')[0]+\\\n",
    "                                                  \"_{}_{}\".format(count_x,count_y)+'_mask.jpg')\n",
    "            # Format of mask_ID now: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_<count_x>_<count_y>_mask.jpg\n",
    "                    cv2.imwrite(write_path_mask,temp_mask)\n",
    "                else:\n",
    "                    write_path_discard=os.path.join(DISCARD_WRITE_PATH,mask_ID.split('.')[0]+\\\n",
    "                                                  \"{}_{}.jpg\".format(count_x,count_y))\n",
    "                    cv2.imwrite(write_path_discard,temp_mask)\n",
    "\n",
    "#extract_patches(MASK_PATH,IMAGE_PATH,IMAGE_WRITE_PATH,MASK_WRITE_PATH,DISCARD_WRITE_PATH)\n",
    "\n",
    "\n",
    "\n",
    "#extract_patches(MASK_PATH_NN,IMAGE_PATH_NN,IMAGE_WRITE_PATH_NN,MASK_WRITE_PATH_NN,DISCARD_WRITE_PATH_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !rm -rf mask_cn_overlap_test\n",
    "# !rm -rf mask_cn_overlap_train\n",
    "# !rm -rf mask_cn_overlap_val\n",
    "\n",
    "# !rm -rf input_cn_overlap_train\n",
    "# !rm -rf input_cn_overlap_test\n",
    "# !rm -rf input_cn_overlap_val\n",
    "# !rm -rf discard_cn_overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_patches(list_images,image_write_path,mask_write_path,discard_path):\n",
    "    MASK_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_mask'\n",
    "    IMAGE_PATH_NOR='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1'\n",
    "    \n",
    "    for image_ID in tqdm(list_images):\n",
    "        # Format of image_ID : D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942.jpg\n",
    "        image_name=os.path.join(IMAGE_PATH_NOR,image_ID)\n",
    "        gray_img=cv2.imread(image_name,0)\n",
    "        bgr_img=cv2.imread(image_name,1)\n",
    "        mask_ID=image_ID.split('.')[0]+'_mask.'+image_ID.split('.')[-1]\n",
    "        # Format of mask_ID: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_mask.jpg\n",
    "        mask_name=os.path.join(MASK_PATH,mask_ID)\n",
    "        mask_img=cv2.imread(mask_name,0)\n",
    "        ret,mask_img=cv2.threshold(mask_img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "\n",
    "        th,img_otsu=cv2.threshold(gray_img,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        x_range=(img_otsu.shape[0]-512)//256 +1\n",
    "        y_range=(img_otsu.shape[1]-512)//256 +1\n",
    "    \n",
    "        for count_x in (range(x_range)):\n",
    "            start_x_pos=count_x*256\n",
    "            end_x_pos=start_x_pos+512\n",
    "            for count_y in (range(y_range)):\n",
    "                start_y_pos=count_y*256\n",
    "                end_y_pos=start_y_pos+512\n",
    "                \n",
    "\n",
    "                temp_otsu=img_otsu[start_x_pos:end_x_pos,start_y_pos:end_y_pos]\n",
    "                temp_mask=mask_img[start_x_pos:end_x_pos,start_y_pos:end_y_pos]\n",
    "\n",
    "\n",
    "                if len(np.where(temp_otsu==0)[0])<0.6*len(np.where(temp_otsu==255)[0]) and len(np.where(temp_mask==255)[0])>=5000:\n",
    "                    img_write=bgr_img[start_x_pos:end_x_pos,start_y_pos:end_y_pos,:]\n",
    "\n",
    "                    write_path_image=os.path.join(image_write_path,image_ID.split('.')[0]+\\\n",
    "                                              \"_{}_{}.jpg\".format(count_x,count_y))\n",
    "            # Format of image_ID now: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_<count_x>_<count_y>.jpg\n",
    "                    cv2.imwrite(write_path_image,img_write)\n",
    "                    write_path_mask=os.path.join(mask_write_path,mask_ID.split('_mask')[0]+\\\n",
    "                                                  \"_{}_{}\".format(count_x,count_y)+'_mask.jpg')\n",
    "            # Format of mask_ID now: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_<count_x>_<count_y>_mask.jpg\n",
    "                    cv2.imwrite(write_path_mask,temp_mask)\n",
    "                else:\n",
    "                    write_path_discard=os.path.join(discard_path,mask_ID.split('.')[0]+\\\n",
    "                                                  \"{}_{}.jpg\".format(count_x,count_y))\n",
    "                    cv2.imwrite(write_path_discard,temp_mask)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "def extract_overlap_patches(MASK_PATH,IMAGE_PATH,IMAGE_WRITE_PATH,MASK_WRITE_PATH,DISCARD_WRITE_PATH):\n",
    "    must_include=['18-00530B_2019-05-07 23_56_22-lv1-11712-16122-7372-7686.jpg',\\\n",
    "        '18-01080B_2019-05-07 21_33_52-lv1-15262-19621-5715-4803.jpg',\\\n",
    "        '18-03912A_2019-05-07 22_55_07-lv1-16713-11566-3121-5791.jpg',\\\n",
    "        '18-04842A_2019-05-07 23_36_39-lv1-21930-4017-5709-5966.jpg',\\\n",
    "        '18-09926A_2019-05-08 00_06_27-lv1-23990-14292-4181-5408.jpg',\\\n",
    "        '18-13347A_2019-05-08 01_31_58-lv1-14057-17758-5693-6244.jpg',\\\n",
    "        '2018_68099_1-1_2019-02-20 23_21_13-lv1-7948-15919-4988-5294.jpg',\\\n",
    "        '2018_72876_1-1_2019-02-21 00_28_52-lv1-31103-26670-3231-3747.jpg',\\\n",
    "        '2018_73834_1-1_2019-02-21 00_16_28-lv1-43278-10364-7406-7672.jpg',\\\n",
    "        '2018_74969_1-1_2019-02-21 00_48_39-lv1-39175-17160-3764-3967.jpg',\\\n",
    "        '2018_83220_1-1_2019-02-20 18_33_11-lv1-58151-37497-5712-3460.jpg',\\\n",
    "        '2019_01246_1-1_2019-02-20 19_23_52-lv1-41847-6127-5995-4660.jpg',\\\n",
    "        '2019_02170_1-2_2019-02-20 19_37_17-lv1-18315-29727-7652-5581.jpg',\\\n",
    "        '2019_03867_1-1_2019-02-20 20_00_32-lv1-36172-15430-6105-5822.jpg',\\\n",
    "        '2019_05944_1-1_2019-02-20 20_42_06-lv1-31069-20357-7018-5934.jpg',\\\n",
    "        '1901940-1_2019-04-30 10_30_47-lv1-39947-11074-3848-5785.jpg',\\\n",
    "        '1800883002_2019-04-30 09_57_31-lv1-28885-30819-3472-3263.jpg',\\\n",
    "        'D201710920_2019-05-21 11_52_57-lv1-33661-31501-7087-7660.jpg',\\\n",
    "        'D201711541_2019-05-21 11_28_51-lv1-39189-27851-3979-5553.jpg',\\\n",
    "        'D201802733_2019-05-14 15_41_01-lv1-18828-14936-7756-5310.jpg']\n",
    "    \n",
    "    image_path_list=[IMAGE_WRITE_PATH+'_train',IMAGE_WRITE_PATH+'_val',IMAGE_WRITE_PATH+'_test']\n",
    "    for write_path in image_path_list:\n",
    "        file_name=write_path.split('/')[-1]\n",
    "        if not os.path.exists(write_path):\n",
    "            os.mkdir(write_path)\n",
    "            print('{} directory made'.format(file_name))\n",
    "        else:\n",
    "            print('{} directory exists'.format(file_name))\n",
    "        \n",
    "   \n",
    "    mask_path_list=[MASK_WRITE_PATH+'_train',MASK_WRITE_PATH+'_val',MASK_WRITE_PATH+'_test']\n",
    "\n",
    "    for write_path in mask_path_list:\n",
    "        file_name=write_path.split('/')[-1]\n",
    "        if not os.path.exists(write_path):\n",
    "            os.mkdir(write_path)\n",
    "            print('{} directory made'.format(file_name))\n",
    "        else:\n",
    "            print('{} directory exists'.format(file_name))\n",
    "\n",
    "    if not os.path.exists(DISCARD_WRITE_PATH):\n",
    "        os.mkdir(DISCARD_WRITE_PATH)\n",
    "        print('discard directory made')\n",
    "    else:\n",
    "        print('discard directory exists')\n",
    "\n",
    "    image_list=os.listdir(IMAGE_PATH)\n",
    "    for x in must_include:\n",
    "        image_list.remove(x)\n",
    "    \n",
    "    \n",
    "    random.shuffle(image_list)\n",
    "    train_list_temp=image_list[:int(0.80*len(image_list))]\n",
    "\n",
    "    train_list=train_list_temp[:int(0.80*len(train_list_temp))]\n",
    "    train_list.extend(must_include)\n",
    "    validation_list=train_list_temp[int(0.80*len(train_list_temp)):]\n",
    "    test_list=image_list[int(0.80*len(image_list)):]\n",
    "    print('TRAIN PATCHES')\n",
    "    gen_patches(train_list,IMAGE_WRITE_PATH+'_train',MASK_WRITE_PATH+'_train',DISCARD_WRITE_PATH)\n",
    "    print('VALIDATION PATCHES')\n",
    "    gen_patches(validation_list,IMAGE_WRITE_PATH+'_val',MASK_WRITE_PATH+'_val',DISCARD_WRITE_PATH)\n",
    "    print('TEST PATCHES')\n",
    "    gen_patches(test_list,IMAGE_WRITE_PATH+'_test',MASK_WRITE_PATH+'_test',DISCARD_WRITE_PATH)\n",
    "\n",
    "\n",
    "                    \n",
    "MASK_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_mask'\n",
    "IMAGE_PATH_NOR='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_color_normalized'\n",
    "IMAGE_PATH_NN='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1'\n",
    "IMAGE_WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/input_overlap'\n",
    "MASK_WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/mask_overlap'\n",
    "DISCARD_WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/discard_overlap'\n",
    "\n",
    "\n",
    "#extract_overlap_patches(MASK_PATH,IMAGE_PATH_NN,IMAGE_WRITE_PATH,MASK_WRITE_PATH,DISCARD_WRITE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_imgs():\n",
    "    MASK_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_mask'\n",
    "    IMAGE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1'\n",
    "    WRITE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/masked_img'\n",
    "    if not os.path.exists(WRITE_PATH):\n",
    "        os.mkdir(WRITE_PATH)\n",
    "        print('{} directory made'.format(WRITE_PATH))\n",
    "    else:\n",
    "        print('{} directory exists'.format(WRITE_PATH))\n",
    "\n",
    "\n",
    "    image_list=os.listdir(IMAGE_PATH)\n",
    "\n",
    "    for image_ID in tqdm(image_list):\n",
    "        # Format of image_ID : D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942.jpg\n",
    "        image_name=os.path.join(IMAGE_PATH,image_ID)\n",
    "        bgr_img=cv2.imread(image_name,1)\n",
    "        mask_ID=image_ID.split('.')[0]+'_mask.'+image_ID.split('.')[-1]\n",
    "        # Format of mask_ID: D20180056701_2019-05-21 09_50_58-lv1-27237-28645-8099-3942_mask.jpg\n",
    "        mask_name=os.path.join(MASK_PATH,mask_ID)\n",
    "        mask_img=cv2.imread(mask_name,0)\n",
    "        ret,mask_img=cv2.threshold(mask_img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "        temp_x,temp_y=np.where(mask_img==0)\n",
    "        bgr_img[temp_x,temp_y,:]=0\n",
    "        cv2.imwrite(os.path.join(WRITE_PATH,image_ID),bgr_img)\n",
    "#create_masked_imgs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,image_IDs,IMAGE_PATH,MASK_PATH,batch_size=32,sample_wise=False,feature_wise=False,scale=1,\\\n",
    "                 shuffle=True):\n",
    "        print(\"Number of sample : {}\".format(len(image_IDs)))\n",
    "\n",
    "        self.image_IDs = image_IDs\n",
    "        self.IMAGE_PATH=IMAGE_PATH\n",
    "        self.MASK_PATH=MASK_PATH\n",
    "        self.batch_size=batch_size\n",
    "        self.sample_wise=sample_wise\n",
    "        self.feature_wise=feature_wise\n",
    "        self.scale=scale\n",
    "        self.shuffle = shuffle\n",
    "        if self.feature_wise==True:\n",
    "            self.mean_value=np.zeros(3)#np.array([154.55561068,100.37918644,167.97885898])\n",
    "            self.std_value=np.ones(3)#np.array([50.64775462,55.05548022,36.14331006])\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_IDs) / self.batch_size))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image_ID_temp= self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for ID in image_ID_temp:\n",
    "            x, y = self.__data_generation(ID)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            \n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indices = np.arange(len(self.image_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "\n",
    "    def __data_generation(self, image_ID_temp):\n",
    "        \n",
    "        img_path=os.path.join(self.IMAGE_PATH,self.image_IDs[image_ID_temp])\n",
    "        \n",
    "        mask_name=self.image_IDs[image_ID_temp].split('.')[0]+'_mask.jpg'\n",
    "        mask_path=os.path.join(self.MASK_PATH,mask_name)\n",
    "        \n",
    "        X = imread(img_path)\n",
    "        #X=resize(X,(1024,1024,3))\n",
    "        \n",
    "        if self.scale!=1 and self.sample_wise==True:\n",
    "            sys.exit(\"Scale and sample_wise, both cannot be implemneted\")\n",
    "        \n",
    "        X=X*self.scale\n",
    "        \n",
    "        if self.sample_wise==True:\n",
    "            X=np.array(X,np.float64)\n",
    "            X[:,:,0]=(X[:,:,0]-np.mean(X[:,:,0]))/np.std(X[:,:,0])\n",
    "            X[:,:,1]=(X[:,:,1]-np.mean(X[:,:,1]))/np.std(X[:,:,1])\n",
    "            X[:,:,2]=(X[:,:,2]-np.mean(X[:,:,2]))/np.std(X[:,:,2])\n",
    "            \n",
    "        if self.feature_wise==True:\n",
    "            X=np.array(X,np.float64)\n",
    "            X=(X-self.mean_value)/self.std_value\n",
    "        #X=np.expand_dims(X,axis=0)\n",
    "        y=imread(mask_path,as_gray=True)\n",
    "        y=y/255\n",
    "        y=np.expand_dims(y,axis=2)\n",
    "\n",
    "\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_array(): \n",
    "    x=[]\n",
    "    y=[]\n",
    "    INPUT_IMAGE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/input_non_normalized'\n",
    "    OUTPUT_MASK_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/mask_non_normalized'\n",
    "    sample_wise=True\n",
    "    imge_name=os.listdir(INPUT_IMAGE_PATH)\n",
    "    for name in tqdm(imge_name):\n",
    "        img=imread(os.path.join(INPUT_IMAGE_PATH,name))\n",
    "        img=np.array(img,np.float64)\n",
    "        if sample_wise==True:\n",
    "                img[:,:,0]=(img[:,:,0]-np.mean(img[:,:,0]))/np.std(img[:,:,0])\n",
    "                img[:,:,1]=(img[:,:,1]-np.mean(img[:,:,1]))/np.std(img[:,:,1])\n",
    "                img[:,:,2]=(img[:,:,2]-np.mean(img[:,:,2]))/np.std(img[:,:,2])\n",
    "        mask_name=name.split('.')[0]+'_mask.jpg'\n",
    "        mask_path=os.path.join(OUTPUT_MASK_PATH,mask_name)\n",
    "        mask=imread(mask_path)\n",
    "        mask=np.array(mask)\n",
    "        mask=np.expand_dims(mask,axis=2)\n",
    "        x.append(img)\n",
    "        y.append(mask/255)\n",
    "\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    print(x.shape,y.shape)\n",
    "    train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.3,shuffle=True,random_state=42)  \n",
    "    return train_x,test_x,train_y,test_y\n",
    "\n",
    "#train_x,test_x,train_y,test_y=train_test_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(train_list):  \n",
    "    IMAGE_PATH_NOR='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1'\n",
    "    INPUT_IMAGE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/input_cn_overlap_train'\n",
    "    \n",
    "    x_sum=np.zeros((512,512,3))\n",
    "    for img in tqdm(train_list):\n",
    "        path=os.path.join(INPUT_IMAGE_PATH,img)\n",
    "        x_sum=x_sum+imread(path)\n",
    "    img_sum=np.sum(x_sum,axis=(0,1))\n",
    "    img_mean=img_sum/(len(train_list)*(512*512))    \n",
    "    x_var=np.zeros((512,512,3))\n",
    "    for img in tqdm(train_list):\n",
    "        path=os.path.join(INPUT_IMAGE_PATH,img)\n",
    "        x_var+=np.square(imread(path)-img_mean)\n",
    "    img_var=np.sum(x_var,axis=(0,1))\n",
    "    img_var=img_var/(len(train_list)*(512*512))\n",
    "    img_std=np.sqrt(img_var)\n",
    "    \n",
    "    \n",
    "    #img_mean=np.array([172.74560205,110.46180509,175.13698655])\n",
    "    #img_std=np.array([41.87534665,51.27207546,35.44355781])\n",
    "    print(img_mean,img_std)\n",
    "    return img_mean,img_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_PATH_TRAIN='input_cn_overlap_train'\n",
    "INPUT_IMAGE_PATH_VAL='input_cn_overlap_val'\n",
    "\n",
    "OUTPUT_MASK_PATH_TRAIN='mask_cn_overlap_train'\n",
    "OUTPUT_MASK_PATH_VAL='mask_cn_overlap_val'\n",
    "\n",
    "train_list=os.listdir(INPUT_IMAGE_PATH_TRAIN)\n",
    "\n",
    "val_list=os.listdir(INPUT_IMAGE_PATH_VAL)\n",
    "train_generator=DataGenerator(train_list,INPUT_IMAGE_PATH_TRAIN,OUTPUT_MASK_PATH_TRAIN,batch_size=8\\\n",
    "                              ,feature_wise=False,scale=1/255)\n",
    "val_generator=DataGenerator(val_list,INPUT_IMAGE_PATH_VAL,OUTPUT_MASK_PATH_VAL,batch_size=8,\\\n",
    "                            feature_wise=False,scale=1/255)\n",
    "hard_code_val=False\n",
    "\n",
    "\n",
    "if hard_code_val:\n",
    "    train_generator.mean_value=[172.4971903,110.05776942,174.9252121] \n",
    "    train_generator.std_value=[41.74648248,50.71953652,35.17684964]\n",
    "\n",
    "    val_generator.mean_value=[172.4971903,110.05776942,174.9252121] \n",
    "    val_generator.std_value=[41.74648248,50.71953652,35.17684964]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_generator.feature_wise and not hard_code_val:\n",
    "    print('Changing mean and std')\n",
    "    val_generator.feature_wise=True\n",
    "    mean,std=get_mean_std(train_list)\n",
    "    train_generator.mean_value=mean\n",
    "    train_generator.std_value=std\n",
    "    val_generator.mean_value=mean\n",
    "    val_generator.std_value=std\n",
    "else:\n",
    "    print(\"Not Changing mean and std\")\n",
    "    #print(train_generator.mean_value,train_generator.std_value)\n",
    "    #print(val_generator.mean_value,val_generator.std_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean and std for overlap [172.74560205,110.46180509,175.13698655] [41.87534665,51.27207546,35.44355781]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(index): \n",
    "    g_x,g_y=train_generator.__getitem__(index)\n",
    "    for i in range(g_x.shape[0]):\n",
    "        print(g_x[i].shape)\n",
    "    print(\"Shape of batches of input: {} \\nShape of batches of output: {}\".format(g_y.shape,g_x.shape))\n",
    "    g_x=g_x[0]\n",
    "    g_y=g_y[0]\n",
    "    print(np.amax(g_y),np.amax(g_x))\n",
    "    print(train_generator.feature_wise)\n",
    "    if train_generator.feature_wise==True:\n",
    "        mean_value=train_generator.mean_value\n",
    "        std_value=train_generator.std_value\n",
    "    else:\n",
    "        mean_value=np.zeros(3)\n",
    "        std_value=np.ones(3)\n",
    "    scale=train_generator.scale\n",
    "        \n",
    "    g_x=(g_x*std_value+mean_value)/scale\n",
    "    g_x=g_x.astype(np.uint8)\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    plt.imshow(g_x)\n",
    "    fig2=plt.figure()\n",
    "    plt.imshow(np.squeeze(g_y,axis=2),cmap=plt.cm.gray)\n",
    "    print(np.amax(g_y),np.amax(g_x))\n",
    "visualize_data(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestart(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=3,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        \n",
    "class CheckLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs):\n",
    "        print(\"LR : {} for epoch {}\".format(K.eval(self.model.optimizer.lr),epoch))\n",
    "        \n",
    "def dice_coef(y_true, y_pred):\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.round(K.flatten(y_pred))\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    union=K.sum(y_true_f) + K.sum(y_pred_f)\n",
    "    \n",
    "    return (2. * intersection) / ( union+ K.epsilon())\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    tp = K.sum(y_true_f * y_pred_f)\n",
    "    fn=K.sum((1-y_pred_f)*y_true_f)\n",
    "    recall= tp/(tp+fn+K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    #y_true_f = K.flatten(y_true)\n",
    "    #y_pred_f = K.flatten(y_pred)\n",
    "    auc = tf.metrics.auc(y_true, y_pred,curve='PR',summation_method=\"careful_interpolation\")[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def cm (y_true, y_pred):\n",
    "    \n",
    "    auc = tf.metrics.auc(y_true, y_pred,curve='PR',summation_method=\"careful_interpolation\")[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    union=K.sum(y_true_f) + K.sum(y_pred_f)\n",
    "    \n",
    "    dice= (2. * intersection) / ( union+ K.epsilon())\n",
    "    \n",
    "    return (dice*100+auc*100)/2\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def unet(pretrained_weights = None,input_size = (None,None,3),act='relu',optimizer='sgd'):\n",
    "    inputs = Input(input_size)\n",
    "    assert act in ['relu','leaky_relu']\n",
    "    assert optimizer in ['adam','sgd']\n",
    "    if act=='relu':\n",
    "        activation1='relu'\n",
    "    else:\n",
    "        activation1=LeakyReLU(alpha=0.5)\n",
    "    activation='relu'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    conv1 = Conv2D(64, 3, activation = activation1, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    #bach_size x 512 x 512 x 64\n",
    "    conv1 = Conv2D(64, 3, activation = activation1, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    #bach_size x 512 x 512 x 64\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #bach_size x 256 x 256 x 64\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #bach_size x 128 x 128 x 128\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #bach_size x 64 x 64 x 256\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    \n",
    "    #pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    #up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    \n",
    "    \n",
    "    up7 = Conv2D(256, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))#(conv6))\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #bach_size x 512 x 512 x 64\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "     #bach_size x 512 x 512 x 128\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(2, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 2\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "     #bach_size x 512 x 512 x 1\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "    \n",
    "    if optimizer=='adam':\n",
    "        OPT=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000, amsgrad=False)\n",
    "        print('adam optimizer')\n",
    "    elif optimizer=='sgd':\n",
    "        OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "        print('SGD optimizer')\n",
    "    #dice_coef_loss\n",
    "    model.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef,recall,auc])    \n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expend_as(tensor, rep):\n",
    "        my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep})(tensor)\n",
    "        return my_repeat\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape):\n",
    "    shape_x = K.int_shape(x)  # 32\n",
    "    shape_g = K.int_shape(g)  # 16\n",
    "\n",
    "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same')(phi_g)  # 16\n",
    "\n",
    "    concat_xg = add([upsample_g, theta_x])\n",
    "    act_xg = Activation('relu')(concat_xg)\n",
    "    psi = Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    # my_repeat=Lambda(lambda xinput:K.repeat_elements(xinput[0],shape_x[1],axis=1))\n",
    "    # upsample_psi=my_repeat([upsample_psi])\n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3])\n",
    "\n",
    "    y = multiply([upsample_psi, x])\n",
    "\n",
    "    # print(K.is_keras_tensor(upsample_psi))\n",
    "\n",
    "    result = Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
    "    result_bn = BatchNormalization()(result)\n",
    "    return result_bn\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm=True):\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3] * 2, (1, 1), strides=(1, 1), padding=\"same\")(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def attn_unet(pretrained_weights = None,input_size = (512,512,3),act='relu',optimizer='adam'):\n",
    "    inputs = Input(input_size)\n",
    "    assert act in ['relu','leaky_relu']\n",
    "    assert optimizer in ['adam','sgd']\n",
    "    if act=='relu':\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "    else:\n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1=LeakyReLU(alpha=0.5)(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1=LeakyReLU(alpha=0.5)(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "    activation='relu'\n",
    "  \n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #bach_size x 256 x 256 x 64\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #bach_size x 128 x 128 x 128\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #bach_size x 64 x 64 x 256\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    \n",
    "    #pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    #up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    \n",
    "    \n",
    "    g1=UnetGatingSignal(drop4)\n",
    "    #bach_size x 64 x 64 x 1024\n",
    "    att1=AttnGatingBlock(conv3, g1, 512)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    up7 = Conv2D(256, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))#(conv6))\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    merge7 = concatenate([att1,up7], axis = 3)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    \n",
    "    g2=UnetGatingSignal(conv7)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    att2=AttnGatingBlock(conv2, g2, 256)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    merge8 = concatenate([att2,up8], axis = 3)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    \n",
    "    g3=UnetGatingSignal(conv8)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    att3=AttnGatingBlock(conv1, g3, 128)\n",
    "    #bach_size x 512 x 512 x 64\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #bach_size x 512 x 512 x 64\n",
    "    merge9 = concatenate([att3,up9], axis = 3)\n",
    "     #bach_size x 512 x 512 x 128\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(2, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 2\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "     #bach_size x 512 x 512 x 1\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "    \n",
    "    if optimizer=='adam':\n",
    "        OPT=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000, amsgrad=False)\n",
    "        print('adam optimizer')\n",
    "    elif optimizer=='sgd':\n",
    "        OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "        print('SGD optimizer')\n",
    "    model.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=attn_unet(input_size = (512,512,3),act='relu',optimizer='sgd')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS=train_generator.__len__()\n",
    "VAL_STEPS=val_generator.__len__()\n",
    "\n",
    "model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "dir_name = os.path.join(os.getcwd(),\"model_non_overlap_unet{}\".format(model_start_date))\n",
    "\n",
    "if os.path.exists(dir_name):\n",
    "    print(\"model already exists in {}\".format(\"model_non_overlap_unet{}\".format(model_start_date)))\n",
    "else:\n",
    "    os.mkdir(dir_name)\n",
    "    print(\" dir {} made\".format(\"model_non_overlap_unet{}\".format(model_start_date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_on_plateau_cb = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,mode='min',min_delta=0.001,\\\n",
    "                     patience=4,cooldown=0,min_lr= 1e-12,verbose=1)\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 2\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "lr_sceduler=keras.callbacks.LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "model_checkpoint_cb=keras.callbacks.ModelCheckpoint('{}/model_optimum_{}.hdf5'.\\\n",
    "                                      format(dir_name,datetime.datetime.now().strftime(\"%Y_%m_%d\")),\\\n",
    "                                      monitor='val_dice_coef', verbose=1, save_best_only=True, \\\n",
    "                                      save_weights_only=True, mode='max', period=1)\n",
    "\n",
    "\n",
    "cosine_annealing_cb=CosineWithRestart(1e-06,0.01,TRAIN_STEPS,lr_decay=1,cycle_length=8,mult_factor=1.5)\n",
    "print_lr_cb=CheckLR()\n",
    "callbacks=[model_checkpoint_cb,print_lr_cb,cosine_annealing_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "history=model.fit_generator(generator=train_generator,epochs=38,verbose=1,\\\n",
    "                            callbacks=callbacks,\\\n",
    "                                 validation_data=val_generator,use_multiprocessing=True)\n",
    "#model.fit(train_x, train_y, epochs=600, batch_size=1, verbose=1, shuffle=False,validation_data=(test_x,test_y))\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['loss'],label='training_loss')\n",
    "plt.plot(history.history['val_loss'],label='validation_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "fig.savefig('{}/plot_loss_{}.PNG'.format(dir_name,model_start_date))\n",
    "\n",
    "f1=history.history['dice_coef']\n",
    "f1_test=history.history['val_dice_coef']\n",
    "\n",
    "fig2=plt.figure()\n",
    "plt.plot(f1,label='training')\n",
    "plt.plot(f1_test,label='testing')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice_score')\n",
    "fig2.savefig('{}/plot_dice_coef_{}.PNG'.format(dir_name,model_start_date))\n",
    "\n",
    "fig3=plt.figure()\n",
    "plt.plot(history.history['recall'],label='training')\n",
    "plt.plot(history.history['val_recall'],label='testing')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('recall')\n",
    "fig3.savefig('{}/plot_recall_{}.PNG'.format(dir_name,model_start_date))\n",
    "\n",
    "fig4=plt.figure()\n",
    "plt.plot(history.history['auc'],label='training')\n",
    "plt.plot(history.history['val_auc'],label='testing')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "fig4.savefig('{}/plot_auc_{}.PNG'.format(dir_name,model_start_date))\n",
    "\n",
    "print(\"Training dice score: {}\\nValidation dice score: {}\".format(f1[-1],f1_test[-1]))\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"{}/model_{}.json\".format(dir_name,model_start_date), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"{}/model_final_weights_{}.h5\".format(dir_name,model_start_date))\n",
    "print(\"Saved model to disk\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_PATH_TEST='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/input_overlap_test/'\n",
    "OUTPUT_MASK_PATH_TEST='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/mask_overlap_test/'\n",
    "\n",
    "test_list=os.listdir(INPUT_IMAGE_PATH_TEST)\n",
    "test_generator=DataGenerator(test_list,INPUT_IMAGE_PATH_TEST,OUTPUT_MASK_PATH_TEST,\\\n",
    "                            batch_size=8,feature_wise=False,scale=1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_content=os.listdir()\n",
    "model_list=[s for s in dir_content if 'model_non_cn' in s]\n",
    "model_list=sorted(model_list)\n",
    "model_folder=os.path.join(os.getcwd(),model_list[-1])\n",
    "print(\"available models are :\",model_list,\"\\n\",\"selected model is in :\", model_folder)\n",
    "model_weights=os.path.join(os.getcwd(),model_folder)+\"/\"+[weights for weights in os.listdir(model_folder) \\\n",
    "                                                          if '.hdf5' in weights][-1]\n",
    "model_architecture=glob.glob(os.path.join(os.getcwd(),model_folder)+\"/*.json\")[-1]\n",
    "print(\"Selected model file is : {}\\nSelected architecture file is : {}\".format(model_weights.split('/')[-1]\\\n",
    "                                                                          ,model_architecture.split('/')[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(model_architecture, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "loaded_model.load_weights(model_weights)\n",
    "loaded_model.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef,recall])\n",
    "print(loaded_model.metrics_names)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_dice_coef, test_recall=loaded_model.evaluate_generator(test_generator,verbose=1,use_multiprocessing=True)\n",
    "print(\"Test loss: {}\\nTest accuracy : {}\\nTest dice score : {}\\nTest recall : {}\"\\\n",
    "      .format(test_loss,test_acc,test_dice_coef,test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import img_as_uint\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "def combined_metric(y_true, y_pred):\n",
    "    \n",
    "    y_true_f = (y_true).reshape((-1,1))\n",
    "    y_pred_f = np.round((y_pred).reshape((-1,1)))\n",
    "    \n",
    "\n",
    "    intersection = np.nansum(y_true_f * y_pred_f)+(np.finfo(float).eps)\n",
    "    union=np.nansum(y_true_f) + np.nansum(y_pred_f)\n",
    "    \n",
    "    dice= (2 * intersection) / ( union+ (np.finfo(float).eps))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dice\n",
    "\n",
    "def whole_img_pred(IMAGE_PATH,img_list,pred_dir_name,MASK_PATH=None,cal_performace=True):\n",
    "    \n",
    "    pred_dir=os.path.join(os.getcwd(),pred_dir_name)\n",
    "    \n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.mkdir(pred_dir)\n",
    "        print(\"Made {} directory\".format(pred_dir.split('/')[-1]))\n",
    "    else:\n",
    "        print(\"{} directory already exists in {}\".format(pred_dir.split('/')[-1],'/'.join(pred_dir.split('/')[:-1])))\n",
    "        \n",
    "        \n",
    "\n",
    "    avg_score=0\n",
    "    test_num=len(img_list)\n",
    "    for img_name in img_list:\n",
    "        str_time=time.time()\n",
    "\n",
    "        img=imread(os.path.join(IMAGE_PATH,img_name))\n",
    "        r,c,_=img.shape#4663,3881\n",
    "      \n",
    "        new_r_count=(math.ceil((r-512)/512)+1)#10\n",
    "        new_c_count=(math.ceil((c-512)/512)+1)#8\n",
    "\n",
    "\n",
    "        pad_r1=((new_r_count-1)*512-r+512)//2 #228\n",
    "        pad_r2=((new_r_count-1)*512-r+512)-pad_r1 #229\n",
    "        pad_c1=((new_c_count-1)*512-c+512)//2 #107\n",
    "        pad_c2=((new_c_count-1)*512-c+512)-pad_c1#108\n",
    "\n",
    "        arr_in=np.pad(img, [(pad_r1,pad_r2),(pad_c1,pad_c2),(0,0)], 'constant', constant_values=0)/255\n",
    "        window_shape=(512,512,3)\n",
    "        arr_out=skimage.util.view_as_windows(arr_in, window_shape, step=512)\n",
    "        x,y=arr_out.shape[:2]\n",
    "        ar2=arr_out.reshape((-1,512,512,3))\n",
    "        del arr_out,arr_in,img\n",
    "           \n",
    "        y_pred=loaded_model.predict(ar2)\n",
    "        \n",
    "    \n",
    "        img_temp=[]\n",
    "        for i in range(x):\n",
    "            img_temp.append(np.concatenate(y_pred[i*y:(i+1)*y],axis=1))\n",
    "        img_temp=np.array(img_temp,dtype=np.float_)\n",
    "\n",
    "        img_temp=np.concatenate(img_temp,axis=0)\n",
    "        img_temp=img_temp[pad_r1:img_temp.shape[0]-pad_r2,pad_c1:img_temp.shape[1]-pad_c2,:]*255\n",
    "        thresh = threshold_otsu(img_temp)\n",
    "        img_temp = np.array(img_temp > 0.8,dtype=np.float32)\n",
    "        #img_temp=scipy.ndimage.morphology.binary_fill_holes(img_temp)\n",
    "        pred_img_name=img_name.split('.')[0]+'_pred.jpg'\n",
    "        pred_img_path=os.path.join(pred_dir,pred_img_name)\n",
    "        skimage.io.imsave(pred_img_path,img_as_uint(img_temp))\n",
    "        print(\"time for {} shaped {} is {} s\".format((r,c),img_name,time.time()-str_time))\n",
    "        if cal_performace:\n",
    "            mask_path=os.path.join(MASK_PATH,img_name.split('.')[0]+'_mask.jpg')\n",
    "            mask=imread(mask_path,as_gray=True)//255\n",
    "            score=combined_metric(mask, img_temp)\n",
    "            avg_score+=score\n",
    "            print(\"Score : {}\".format(score))\n",
    "        del img_temp,ar2\n",
    "        if cal_performace:\n",
    "            del mask\n",
    "    if cal_performace:\n",
    "        print(\"Average Score : {}\".format(avg_score/test_num))\n",
    "    \n",
    "    print(\"DONE\")\n",
    "        \n",
    "MASK_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_mask'\n",
    "IMAGE_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-pos-v1_color_normalized'\n",
    "\n",
    "#NEG_IMAGE_PATH='/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-neg'\n",
    "#neg_img_list=os.listdir(NEG_IMAGE_PATH)\n",
    "\n",
    "all_img_list=os.listdir(IMAGE_PATH)\n",
    "\n",
    "test_files=os.listdir(INPUT_IMAGE_PATH_TEST)\n",
    "test_img_list=['_'.join(x.split('_')[:-2])+'.jpg' for x in test_files]\n",
    "\n",
    "\n",
    "whole_img_pred(IMAGE_PATH,test_img_list,'whole_pred_vanillaUnet_{}'.format(datetime.datetime.now().strftime(\"%Y_%m_%d\")),\\\n",
    "               MASK_PATH,cal_performace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_PATH='/home/vahadaneabhi01/datalab/digest/Colonoscopy_tissue_segment_dataset/tissue-train-neg/'\n",
    "images=[NEG_PATH+x for x in os.listdir(NEG_PATH)]\n",
    "whole_img_pred(NEG_PATH,images,'whole_pred_neg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performance(index,model):\n",
    "    x,y=test_generator.__getitem__(index)\n",
    "    y_pred=loaded_model.predict(x)\n",
    "    temp=test_generator.indices[index*test_generator.batch_size:(index+1)*test_generator.batch_size]\n",
    "    print(test_generator.image_IDs[temp[0]])\n",
    "    y_pred_temp=np.squeeze(y_pred[0],axis=2)\n",
    "    y_true_temp=np.squeeze(y[0],axis=2)\n",
    "    fig=plt.figure()\n",
    "    plt.imshow(y_true_temp,cmap=plt.cm.gray)\n",
    "    fig1=plt.figure()\n",
    "    plt.imshow(y_pred_temp,cmap=plt.cm.gray)\n",
    "    fig2=plt.figure()\n",
    "    plt.imshow(x[0])\n",
    "    \n",
    "    \n",
    "visualize_performance(12,loaded_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2_folder=os.path.join(os.getcwd(),model_list[-3])\n",
    "\n",
    "model2_weights=os.path.join(os.getcwd(),model_folder)+\"/\"+[weights for weights in os.listdir(model2_folder) \\\n",
    "                                                          if 'model_op' in weights][-1]\n",
    "model2_architecture=glob.glob(os.path.join(os.getcwd(),model2_folder)+\"/*.json\")[-1]\n",
    "\n",
    "json_file2 = open(model2_architecture, 'r')\n",
    "loaded_model_json2 = json_file2.read()\n",
    "json_file2.close()\n",
    "loaded_model2 = model_from_json(loaded_model_json2)\n",
    "OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "loaded_model2.load_weights(model_weights)\n",
    "loaded_model2.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef,auc])\n",
    "print(\"Selected model2 file is : {}\\nSelected architecture2 file is : {}\".format(model2_weights.split('/')[-1]\\\n",
    "                                                                          ,model2_architecture.split('/')[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(12,loaded_model2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
